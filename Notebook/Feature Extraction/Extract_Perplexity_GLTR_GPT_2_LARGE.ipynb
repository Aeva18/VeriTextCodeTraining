{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROSS_ENTROPY = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "sent_split    = PunktSentenceTokenizer().tokenize\n",
    "DEVICE        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def extract_gpt2_features(text, tokenizer, model, sent_cut):\n",
    "    \"\"\"\n",
    "    Compute various perplexity (ppl) metrics and token ranking statistics for a given text using a GPT-2 model.\n",
    "    \"\"\"\n",
    "    input_max_length = tokenizer.model_max_length - 2\n",
    "    token_ids, offsets = [], []\n",
    "    sentences = sent_cut(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        truncation_limit = len(token_ids) + len(ids) - input_max_length\n",
    "        \n",
    "        if truncation_limit > 0:\n",
    "            ids = ids[:-truncation_limit]\n",
    "        \n",
    "        offsets.append((len(token_ids), len(token_ids) + len(ids)))\n",
    "        token_ids.extend(ids)\n",
    "        \n",
    "        if truncation_limit >= 0:\n",
    "            break\n",
    "    \n",
    "    input_ids = torch.tensor([tokenizer.bos_token_id] + token_ids + [tokenizer.eos_token_id]).to(DEVICE)\n",
    "    logits = model(input_ids).logits\n",
    "    \n",
    "    # Shift logits to align with targets\n",
    "    shift_logits = logits[:-1].contiguous()\n",
    "    shift_target = input_ids[1:].contiguous()\n",
    "    loss = CROSS_ENTROPY(shift_logits, shift_target)\n",
    "    \n",
    "    all_probs = torch.softmax(shift_logits, dim=-1)\n",
    "    sorted_ids = torch.argsort(all_probs, dim=-1, descending=True)\n",
    "    expanded_tokens = shift_target.unsqueeze(-1).expand_as(sorted_ids)\n",
    "    indices = torch.where(sorted_ids == expanded_tokens)\n",
    "    rank = indices[-1]\n",
    "    \n",
    "    # Rank distribution counters\n",
    "    rank_counters = [\n",
    "        (rank < 10).long().sum().item(),\n",
    "        ((rank >= 10) & (rank < 100)).long().sum().item(),\n",
    "        ((rank >= 100) & (rank < 1000)).long().sum().item(),\n",
    "        (rank >= 1000).long().sum().item()\n",
    "    ]\n",
    "    \n",
    "    # Compute different levels of perplexity (ppl)\n",
    "    text_ppl = loss.mean().exp().item()\n",
    "    sent_ppl = [(loss[start:end].sum() / (end - start)).exp().item() for start, end in offsets]\n",
    "    \n",
    "    max_sent_ppl = max(sent_ppl)\n",
    "    sent_ppl_avg = sum(sent_ppl) / len(sent_ppl)\n",
    "    sent_ppl_std = torch.std(torch.tensor(sent_ppl)).item() if len(sent_ppl) > 1 else 0\n",
    "    \n",
    "    mask = torch.ones(loss.size(0), device=DEVICE)\n",
    "    step_ppl = loss.cumsum(dim=-1).div(mask.cumsum(dim=-1)).exp()\n",
    "    max_step_ppl = step_ppl.max().item()\n",
    "    step_ppl_avg = step_ppl.mean().item()\n",
    "    step_ppl_std = step_ppl.std().item() if step_ppl.size(0) > 1 else 0\n",
    "    \n",
    "    ppl_metrics = [\n",
    "        text_ppl, max_sent_ppl, sent_ppl_avg, sent_ppl_std,\n",
    "        max_step_ppl, step_ppl_avg, step_ppl_std\n",
    "    ]\n",
    "    \n",
    "    return rank_counters, ppl_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'text_ppl', 'max_sent_ppl', 'sent_ppl_avg', 'sent_ppl_std', 'max_step_ppl', \n",
    "    'step_ppl_avg', 'step_ppl_std', 'rank_0', 'rank_10', 'rank_100', 'rank_1000'\n",
    "]\n",
    "\n",
    "for i in range(100,101):\n",
    "    curr_num = i\n",
    "    train          = pd.read_csv('../../chunk_test_98.csv')\n",
    "    train['label'] = np.where(train['source'] == 'Human', 0, 1)\n",
    "    models_train_feats = []\n",
    "\n",
    "    TOKENIZER_EN = AutoTokenizer.from_pretrained(\"../../gpt2-large/tokenizer\")\n",
    "    MODEL_EN = AutoModelForCausalLM.from_pretrained(\"../../gpt2-large/model\").to(DEVICE)\n",
    "\n",
    "    train_ppl_feats  = []\n",
    "    train_gltr_feats = []\n",
    "    print(f\"Chunk {curr_num} In Process\")\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(train.text.values):\n",
    "            gltr, ppl = extract_gpt2_features(text, TOKENIZER_EN, MODEL_EN, sent_split)\n",
    "            train_ppl_feats.append(ppl)\n",
    "            train_gltr_feats.append(gltr)\n",
    "\n",
    "    X_train = pd.DataFrame(\n",
    "        np.concatenate((train_ppl_feats, train_gltr_feats), axis=1), \n",
    "        columns=[f'gpt2-large-{col}' for col in cols]\n",
    "    )\n",
    "    models_train_feats.append(X_train)\n",
    "\n",
    "    train_feats = pd.concat(models_train_feats, axis=1)\n",
    "    train_feats = pd.concat([train,train_feats], axis = 1)\n",
    "    train_feats.to_csv(f\"Data/gpt2-large-feature/Split3/chunk_{curr_num}.csv\",index=False)\n",
    "    print(f\"Chunk {curr_num} extracted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
